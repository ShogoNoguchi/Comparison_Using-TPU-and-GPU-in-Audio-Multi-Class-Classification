{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "音声分類onGPU",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShogoNoguchi/Comparison_Using-TPU-and-GPU-in-Audio-Multi-Class-Classification/blob/main/%E9%9F%B3%E5%A3%B0%E5%88%86%E9%A1%9EonGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T06:42:49.656003Z",
          "iopub.execute_input": "2024-12-01T06:42:49.656553Z",
          "iopub.status.idle": "2024-12-01T06:42:50.676367Z",
          "shell.execute_reply.started": "2024-12-01T06:42:49.656525Z",
          "shell.execute_reply": "2024-12-01T06:42:50.675629Z"
        },
        "id": "UafDX2bgL7Q4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchaudio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 必要なディレクトリを作成\n",
        "data_dir = \"data\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# ターゲットラベルの定義\n",
        "target_labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
        "unknown_label = 'unknown'\n",
        "unique_labels = target_labels + [unknown_label]\n",
        "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# データセットの読み込み（公式の分割を使用）\n",
        "train_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, subset='training', download=True)\n",
        "validation_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, subset='validation', download=True)\n",
        "test_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, subset='testing', download=True)\n",
        "\n",
        "class SpeechCommandsDataset(Dataset):\n",
        "    def __init__(self, dataset, sample_rate=16000, n_mfcc=40, max_length=16000, label_map=None):\n",
        "        self.dataset = dataset\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.max_length = max_length\n",
        "        self.label_map = label_map\n",
        "\n",
        "        # 前処理トランスフォーム\n",
        "        self.resample_transform = torchaudio.transforms.Resample(orig_freq=16000, new_freq=sample_rate)\n",
        "        self.mfcc_transform = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=n_mfcc,\n",
        "            melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": n_mfcc},\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 元データを取得\n",
        "        waveform, original_sample_rate, label, _, _ = self.dataset[idx]\n",
        "\n",
        "        # ターゲットラベル以外を\"unknown\"にマッピング\n",
        "        label = label if label in target_labels else unknown_label\n",
        "\n",
        "        # リサンプリング\n",
        "        if original_sample_rate != self.sample_rate:\n",
        "            waveform = self.resample_transform(waveform)\n",
        "\n",
        "        # 標準化\n",
        "        waveform = (waveform - waveform.mean()) / waveform.std()\n",
        "\n",
        "        # 長さ調整\n",
        "        if waveform.size(1) < self.max_length:\n",
        "            padding = self.max_length - waveform.size(1)\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "        else:\n",
        "            waveform = waveform[:, :self.max_length]\n",
        "\n",
        "        # MFCCに変換\n",
        "        mfcc = self.mfcc_transform(waveform)\n",
        "\n",
        "        # ラベルを数値化\n",
        "        label_id = self.label_map[label] if self.label_map else label\n",
        "\n",
        "        return mfcc, label_id\n",
        "\n",
        "class SpeechCommandClassifier(nn.Module):\n",
        "    def __init__(self, n_mfcc=40, num_classes=len(label_map)):\n",
        "        super(SpeechCommandClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # サンプル入力を使用してフラット化後のサイズを計算\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.zeros(1, 1, n_mfcc, 101)\n",
        "            out = self.pool(F.relu(self.conv1(sample_input)))\n",
        "            out = self.pool(F.relu(self.conv2(out)))\n",
        "            flattened_size = out.view(-1).shape[0]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # フラット化\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# データセットのインスタンスを作成\n",
        "train_data = SpeechCommandsDataset(train_dataset, label_map=label_map)\n",
        "validation_data = SpeechCommandsDataset(validation_dataset, label_map=label_map)\n",
        "test_data = SpeechCommandsDataset(test_dataset, label_map=label_map)\n",
        "\n",
        "# データローダーの作成\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# デバイスの設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# モデルのインスタンスを作成し、デバイスに移行\n",
        "model = SpeechCommandClassifier(n_mfcc=40, num_classes=len(label_map)).to(device)\n",
        "\n",
        "# 損失関数とオプティマイザーの定義\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        # 不要な次元の追加を避ける\n",
        "        # X = X.unsqueeze(1)  # この行は不要\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * X.size(0)\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "        total_samples += X.size(0)\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f\"Batch {batch}, Loss: {loss.item():>7f}\")\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = correct / total_samples\n",
        "    print(f\"Train - Avg loss: {avg_loss:>8f}, Accuracy: {(100 * accuracy):>0.1f}%\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            # 不要な次元の追加を避ける\n",
        "            # X = X.unsqueeze(1)  # この行は不要\n",
        "            pred = model(X)\n",
        "            total_loss += loss_fn(pred, y).item() * X.size(0)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            total_samples += X.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = correct / total_samples\n",
        "    print(f\"Test - Avg loss: {avg_loss:>8f}, Accuracy: {(100 * accuracy):>0.1f}%\")\n",
        "\n",
        "# 学習ループ\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loop(train_loader, model, loss_fn, optimizer)\n",
        "    test_loop(validation_loader, model, loss_fn)\n",
        "print(\"訓練完了！\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T06:42:50.677739Z",
          "iopub.execute_input": "2024-12-01T06:42:50.67816Z",
          "iopub.status.idle": "2024-12-01T07:16:47.220081Z",
          "shell.execute_reply.started": "2024-12-01T06:42:50.678103Z",
          "shell.execute_reply": "2024-12-01T07:16:47.218952Z"
        },
        "id": "dNyhpToHL7Q5",
        "outputId": "c584af65-aea2-4843-a234-f2e076a023aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 2.26G/2.26G [00:09<00:00, 245MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1\n-------------------------------\nBatch 0, Loss: 4.286769\nBatch 100, Loss: 1.445020\nBatch 200, Loss: 1.190274\nBatch 300, Loss: 1.205915\nBatch 400, Loss: 0.936719\nBatch 500, Loss: 0.794121\nBatch 600, Loss: 0.670147\nBatch 700, Loss: 0.869510\nBatch 800, Loss: 0.909492\nBatch 900, Loss: 0.641240\nBatch 1000, Loss: 0.605613\nBatch 1100, Loss: 0.526469\nBatch 1200, Loss: 0.308218\nBatch 1300, Loss: 0.542874\nTrain - Avg loss: 0.932779, Accuracy: 72.6%\nTest - Avg loss: 0.563725, Accuracy: 82.4%\nEpoch 2\n-------------------------------\nBatch 0, Loss: 0.541320\nBatch 100, Loss: 0.618990\nBatch 200, Loss: 0.392678\nBatch 300, Loss: 0.439378\nBatch 400, Loss: 0.643299\nBatch 500, Loss: 0.655211\nBatch 600, Loss: 0.520213\nBatch 700, Loss: 0.239718\nBatch 800, Loss: 0.486059\nBatch 900, Loss: 0.494806\nBatch 1000, Loss: 0.546382\nBatch 1100, Loss: 0.394878\nBatch 1200, Loss: 0.261218\nBatch 1300, Loss: 0.227475\nTrain - Avg loss: 0.470116, Accuracy: 84.9%\nTest - Avg loss: 0.458786, Accuracy: 84.8%\nEpoch 3\n-------------------------------\nBatch 0, Loss: 0.509556\nBatch 100, Loss: 0.319753\nBatch 200, Loss: 0.393200\nBatch 300, Loss: 0.404218\nBatch 400, Loss: 0.360727\nBatch 500, Loss: 0.541261\nBatch 600, Loss: 0.339478\nBatch 700, Loss: 0.491781\nBatch 800, Loss: 0.380124\nBatch 900, Loss: 0.121473\nBatch 1000, Loss: 0.525097\nBatch 1100, Loss: 0.143788\nBatch 1200, Loss: 0.270744\nBatch 1300, Loss: 0.579837\nTrain - Avg loss: 0.357496, Accuracy: 88.4%\nTest - Avg loss: 0.399368, Accuracy: 87.0%\nEpoch 4\n-------------------------------\nBatch 0, Loss: 0.304880\nBatch 100, Loss: 0.334940\nBatch 200, Loss: 0.224773\nBatch 300, Loss: 0.232485\nBatch 400, Loss: 0.295221\nBatch 500, Loss: 0.292180\nBatch 600, Loss: 0.185409\nBatch 700, Loss: 0.408311\nBatch 800, Loss: 0.226563\nBatch 900, Loss: 0.141869\nBatch 1000, Loss: 0.209330\nBatch 1100, Loss: 0.524590\nBatch 1200, Loss: 0.287937\nBatch 1300, Loss: 0.241448\nTrain - Avg loss: 0.285098, Accuracy: 90.6%\nTest - Avg loss: 0.412397, Accuracy: 87.2%\nEpoch 5\n-------------------------------\nBatch 0, Loss: 0.313775\nBatch 100, Loss: 0.211475\nBatch 200, Loss: 0.147475\nBatch 300, Loss: 0.145255\nBatch 400, Loss: 0.216631\nBatch 500, Loss: 0.255794\nBatch 600, Loss: 0.196279\nBatch 700, Loss: 0.216890\nBatch 800, Loss: 0.167603\nBatch 900, Loss: 0.208095\nBatch 1000, Loss: 0.496342\nBatch 1100, Loss: 0.278969\nBatch 1200, Loss: 0.192786\nBatch 1300, Loss: 0.098773\nTrain - Avg loss: 0.230763, Accuracy: 92.3%\nTest - Avg loss: 0.403418, Accuracy: 87.9%\n訓練完了！\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}