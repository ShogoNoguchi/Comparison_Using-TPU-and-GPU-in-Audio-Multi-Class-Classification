{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torchaudio\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# TPU用のモジュールをインポート\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n# import torch_xla.runtime as xr  # この行は不要\n\n# 必要なディレクトリを作成\ndata_dir = \"data\"\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n# ターゲットラベルの定義\ntarget_labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\nunknown_label = 'unknown'\nunique_labels = target_labels + [unknown_label]\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\n\n# データセットクラスの定義はそのまま\nclass SpeechCommandsDataset(Dataset):\n    def __init__(self, dataset, sample_rate=16000, n_mfcc=40, max_length=16000, label_map=None):\n        self.dataset = dataset\n        self.sample_rate = sample_rate\n        self.n_mfcc = n_mfcc\n        self.max_length = max_length\n        self.label_map = label_map\n        \n        # 前処理トランスフォーム\n        self.resample_transform = torchaudio.transforms.Resample(orig_freq=16000, new_freq=sample_rate)\n        self.mfcc_transform = torchaudio.transforms.MFCC(\n            sample_rate=sample_rate, \n            n_mfcc=n_mfcc,\n            melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": n_mfcc},\n        )\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        waveform, original_sample_rate, label, _, _ = self.dataset[idx]\n        label = label if label in target_labels else unknown_label\n        if original_sample_rate != self.sample_rate:\n            waveform = self.resample_transform(waveform)\n        waveform = (waveform - waveform.mean()) / waveform.std()\n        if waveform.size(1) < self.max_length:\n            padding = self.max_length - waveform.size(1)\n            waveform = torch.nn.functional.pad(waveform, (0, padding))\n        else:\n            waveform = waveform[:, :self.max_length]\n        mfcc = self.mfcc_transform(waveform)\n        label_id = self.label_map[label] if self.label_map else label\n        return mfcc, label_id\n\n# モデルクラスの定義はそのまま\nclass SpeechCommandClassifier(nn.Module):\n    def __init__(self, n_mfcc=40, num_classes=len(label_map)):\n        super(SpeechCommandClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        \n        # サンプル入力を使用してフラット化後のサイズを計算\n        with torch.no_grad():\n            sample_input = torch.zeros(1, 1, n_mfcc, 101)\n            out = self.pool(F.relu(self.conv1(sample_input)))\n            out = self.pool(F.relu(self.conv2(out)))\n            flattened_size = out.view(-1).shape[0]\n        \n        self.fc1 = nn.Linear(flattened_size, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n    \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)  # フラット化\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# トレーニングループの定義\ndef train_loop(dataloader, model, loss_fn, optimizer, device, epoch):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total_samples = 0\n    for batch_idx, (X, y) in enumerate(dataloader):\n        X = X.to(device)\n        y = y.to(device)\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        # オプティマイザーステップの変更\n        xm.optimizer_step(optimizer)\n        total_loss += loss.item() * X.size(0)\n        correct += (pred.argmax(1) == y).sum().item()\n        total_samples += X.size(0)\n        if batch_idx % 100 == 0 and xm.is_master_ordinal():\n            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():>7f}\")\n    avg_loss = total_loss / total_samples\n    accuracy = correct / total_samples\n    if xm.is_master_ordinal():\n        print(f\"Train - Avg loss: {avg_loss:>8f}, Accuracy: {(100 * accuracy):>0.1f}%\")\n\n# テストループの定義\ndef test_loop(dataloader, model, loss_fn, device, mode='Validation'):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total_samples = 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            pred = model(X)\n            loss = loss_fn(pred, y)\n            total_loss += loss.item() * X.size(0)\n            correct += (pred.argmax(1) == y).sum().item()\n            total_samples += X.size(0)\n    avg_loss = total_loss / total_samples\n    accuracy = correct / total_samples\n    if xm.is_master_ordinal():\n        print(f\"{mode} - Avg loss: {avg_loss:>8f}, Accuracy: {(100 * accuracy):>0.1f}%\")\n\n# マルチプロセッシング用の関数\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    # デバイスの設定\n    device = xm.xla_device()\n    # データセットのインスタンスを作成\n    global train_dataset, validation_dataset, test_dataset\n    train_data = SpeechCommandsDataset(train_dataset, label_map=label_map)\n    validation_data = SpeechCommandsDataset(validation_dataset, label_map=label_map)\n    test_data = SpeechCommandsDataset(test_dataset, label_map=label_map)\n    \n    # バッチサイズの設定（必要に応じて調整）\n    batch_size = 64\n    # データローダーの作成\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n    \n    # データローダーをTPUデバイス用にラップ\n    train_loader = pl.MpDeviceLoader(train_loader, device)\n    validation_loader = pl.MpDeviceLoader(validation_loader, device)\n    test_loader = pl.MpDeviceLoader(test_loader, device)\n    \n    # モデルのインスタンスを作成し、デバイスに移行\n    model = SpeechCommandClassifier(n_mfcc=40, num_classes=len(label_map)).to(device)\n    # 損失関数とオプティマイザーの定義\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    \n    epochs = 5\n    for epoch in range(1, epochs +1):\n        if xm.is_master_ordinal():\n            print(f\"Epoch {epoch}\\n-------------------------------\")\n        train_loop(train_loader, model, loss_fn, optimizer, device, epoch)\n        test_loop(validation_loader, model, loss_fn, device)\n    if xm.is_master_ordinal():\n        print(\"訓練完了！\")\n\nif __name__ == \"__main__\":\n    # データセットをメインプロセスでロード\n    train_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, subset='training', download=True)\n    validation_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, subset='validation', download=True)\n    test_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, subset='testing', download=True)\n    # 利用可能なTPUコア数を取得\n    tpu_cores = xm.xrt_world_size()\n    print(f\"Available TPU cores: {tpu_cores}\")\n    # マルチプロセッシングを開始\n    xmp.spawn(_mp_fn, args=(None,), nprocs=tpu_cores, start_method='fork')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}